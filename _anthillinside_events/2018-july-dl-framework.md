---
layout: workshop
title: "Workshop: Make your own DL framework"
subtitle: Learn to write Deep Learning Framework in Pure Python and numpy
datelocation: "29 July 2018, 09:00 AM - 05:00 PM, Bangalore"
city: Bangalore
start_time: 2018-07-29
end_time: 2018-07-29
description: "We have all used all the high end frameworks that works really well. How about writing a small strip down version of one. In this session, Iâ€™ll walk you through how to write a small Deep Learning Framework in Pure python and numpy which has auto grad and optimizers and easy to create models. Also, the framework will be extendible so that you can easily play around with."
boxoffice_item_collection: '26ac0e6c-4a1d-4b13-ae9c-c8bb372dd1b1'

instructors:
- name: Nithish Divakar
  byline: Computer Vision Research Engineer, Cogknit Semantis
  image_url: https://images.hasgeek.com/embed/file/d157fd364906433aa09c0d89e3c7815c
  website:  
    url: http://everythingproject.in/
    label: Blog
  bio: |
    Nithish Divakar is a Computer Vision Research Engineer at Cogknit Semantics. He Is a masters graduate from IISc bangalore and has been working in deep learning for past 4 years. He has prior published research work in GANs and Image captioning. See everythingproject.in for more.

overview:
  left_content: |
    # Socpe of the Workshop

    This will be a theory and implementation workshop. The theory part will be focused towards explaining the fundamentals and giving reasons for various design decisions of the implementations part. 

    # Sections of the workshop


    1. The basics
    2. Implementing a neural Network
    3. Computational Graph and implementing auto grad
    4. Implementing our very own nabla framework.

    # What do you need to know/have a priori!

    * A laptop with good battery backup
    * A python environment installed and ready to go (python3 and numpy latest versions)
    * Some basic knowledge in how neural networks work


  right_content: |
    # Detailed overview

    ### The basics
    * Parameterized models
    * Gradient Descent
    * Computational view of Neural Network
    * Arriving at backprop algorithm
    
    ### Implementing a neural network
    * Deriving gradients of a network
    * details of implementation
    * Some pitfalls and pointers
    * Why this approach cannot be used to build a framework
    
    ### Computational graph and Implementing auto grad
    * What is Computational Graph?
    * Automatic differentiation
    * Forward and backward accumulation
    
    ### The nabla framework[coding session]
    * Implementing forward computation
    * Implementing tensors, variables and parameters
    * Implementing operations
    * Implementing forward computation
    * Implementing local gradients
    * Implementing backward computation
    * Putting it all together
---
